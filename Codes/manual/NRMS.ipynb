{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import evaluate,train,prepare\n",
    "from models.NRMS import NRMSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'scale':'demo',\n",
    "    'name':'nrms',\n",
    "    'batch_size':5,\n",
    "    'title_size':20,\n",
    "    'his_size':50,\n",
    "    'npratio':4,\n",
    "    'dropout_p':0.2,\n",
    "    'query_dim':200,\n",
    "    'embedding_dim':300,\n",
    "    'value_dim':16,\n",
    "    'head_num':16,\n",
    "    'epochs':1,\n",
    "    'metrics':'auc,group_auc,mean_mrr,ndcg@5,ndcg@10',\n",
    "    'device':'cuda:0',\n",
    "    'attrs': ['title'],\n",
    "    'k':-1,\n",
    "    'select':None,\n",
    "    'save_step':0,\n",
    "    'save_each_epoch':False,\n",
    "    'train_embedding':False,\n",
    "    'mode':'train',\n",
    "    'news_id':False,\n",
    "    'validate':False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(hparams['device'])\n",
    "vocab, loaders = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmsModel = NRMSModel(vocab=vocab,hparams=hparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(nrmsModel, hparams, loaders, interval=10)"
   ]
  },
  {
   "source": [
    "nrmsModel.cdd_size = 1"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cal_metric(labels, preds, metrics):\n",
    "    \"\"\"Calculate metrics,such as auc, logloss.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for metric in metrics:\n",
    "        if metric == \"auc\":\n",
    "            auc = np.mean(\n",
    "                [\n",
    "                    roc_auc_score(each_labels, each_preds)\n",
    "                    for each_labels, each_preds in zip(labels, preds)\n",
    "                ]\n",
    "            )\n",
    "            res[\"auc\"] = round(auc, 4)\n",
    "        elif metric == \"rmse\":\n",
    "            rmse = mean_squared_error(np.asarray(labels), np.asarray(preds))\n",
    "            res[\"rmse\"] = np.sqrt(round(rmse, 4))\n",
    "        elif metric == \"logloss\":\n",
    "            # avoid logloss nan\n",
    "            preds = [max(min(p, 1.0 - 10e-12), 10e-12) for p in preds]\n",
    "            logloss = log_loss(np.asarray(labels), np.asarray(preds))\n",
    "            res[\"logloss\"] = round(logloss, 4)\n",
    "        elif metric == \"acc\":\n",
    "            pred = np.asarray(preds)\n",
    "            pred[pred >= 0.5] = 1\n",
    "            pred[pred < 0.5] = 0\n",
    "            acc = accuracy_score(np.asarray(labels), pred)\n",
    "            res[\"acc\"] = round(acc, 4)\n",
    "        elif metric == \"f1\":\n",
    "            pred = np.asarray(preds)\n",
    "            pred[pred >= 0.5] = 1\n",
    "            pred[pred < 0.5] = 0\n",
    "            f1 = f1_score(np.asarray(labels), pred)\n",
    "            res[\"f1\"] = round(f1, 4)\n",
    "        elif metric == \"mean_mrr\":\n",
    "            mean_mrr = np.mean(\n",
    "                [\n",
    "                    mrr_score(each_labels, each_preds)\n",
    "                    for each_labels, each_preds in zip(labels, preds)\n",
    "                ]\n",
    "            )\n",
    "            res[\"mean_mrr\"] = round(mean_mrr, 4)\n",
    "        elif metric.startswith(\"ndcg\"):  # format like:  ndcg@2;4;6;8\n",
    "            ndcg_list = [1, 2]\n",
    "            ks = metric.split(\"@\")\n",
    "            if len(ks) > 1:\n",
    "                ndcg_list = [int(token) for token in ks[1].split(\";\")]\n",
    "            for k in ndcg_list:\n",
    "                ndcg_temp = np.mean(\n",
    "                    [\n",
    "                        ndcg_score(each_labels, each_preds, k)\n",
    "                        for each_labels, each_preds in zip(labels, preds)\n",
    "                    ]\n",
    "                )\n",
    "                res[\"ndcg@{0}\".format(k)] = round(ndcg_temp, 4)\n",
    "        elif metric.startswith(\"hit\"):  # format like:  hit@2;4;6;8\n",
    "            hit_list = [1, 2]\n",
    "            ks = metric.split(\"@\")\n",
    "            if len(ks) > 1:\n",
    "                hit_list = [int(token) for token in ks[1].split(\";\")]\n",
    "            for k in hit_list:\n",
    "                hit_temp = np.mean(\n",
    "                    [\n",
    "                        hit_score(each_labels, each_preds, k)\n",
    "                        for each_labels, each_preds in zip(labels, preds)\n",
    "                    ]\n",
    "                )\n",
    "                res[\"hit@{0}\".format(k)] = round(hit_temp, 4)\n",
    "        else:\n",
    "            raise ValueError(\"not define this metric {0}\".format(metric))\n",
    "    return res\n",
    "\n",
    "def run_eval(model,dataloader,interval):\n",
    "    \"\"\" making prediction and gather results into groups according to impression_id, display processing every interval batches\n",
    "\n",
    "    Args:\n",
    "        model(torch.nn.Module)\n",
    "        dataloader(torch.utils.data.DataLoader): provide data\n",
    "\n",
    "    Returns:\n",
    "        impression_id: impression ids after group\n",
    "        labels: labels after group.\n",
    "        preds: preds after group.\n",
    "\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    imp_indexes = []\n",
    "    \n",
    "    for i,batch_data_input in tqdm(enumerate(dataloader)):\n",
    "        \n",
    "        preds.extend(model.forward(batch_data_input).tolist())\n",
    "        print(model.forward(batch_data_input).shape)\n",
    "        label = batch_data_input['labels'].squeeze(dim=-1).tolist()\n",
    "        labels.extend(label)\n",
    "        imp_indexes.extend(batch_data_input['impression_index'])\n",
    "    \n",
    "    all_keys = list(set(imp_indexes))\n",
    "    all_keys.sort()\n",
    "    group_labels = {k: [] for k in all_keys}\n",
    "    group_preds = {k: [] for k in all_keys}\n",
    "\n",
    "    for l, p, k in zip(labels, preds, imp_indexes):\n",
    "        group_labels[k].append(l)\n",
    "        group_preds[k].append(p)\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for k in all_keys:\n",
    "        all_labels.append(group_labels[k])\n",
    "        all_preds.append(group_preds[k])\n",
    "    \n",
    "    return group_labels.keys(), all_labels, all_preds\n",
    "\n",
    "def evaluate(model,hparams,dataloader,interval=100):\n",
    "    \"\"\"Evaluate the given file and returns some evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        model(nn.Module)\n",
    "        hparams(dict)\n",
    "        dataloader(torch.utils.data.DataLoader): provide data\n",
    "        interval(int): within each epoch, the interval of steps to display loss\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary contains evaluation metrics.\n",
    "    \"\"\"\n",
    "    hparam_list = ['name','scale','epochs','save_step','train_embedding','select','integrate','his_size','k','query_dim','value_dim','head_num']\n",
    "    param_list = ['query_words','query_levels']\n",
    "    model.eval()\n",
    "    model.cdd_size = 1\n",
    "    imp_indexes, labels, preds = run_eval(model,dataloader,interval)\n",
    "    res = cal_metric(labels,preds,model.metrics.split(','))\n",
    "    print(\"evaluation results:{}\".format(res))\n",
    "    with open('performance.log','a+') as f:\n",
    "        # model_name = '{}-{}_{}_epoch{}_step{}_[hs={},topk={}]:'.format(hparams['name'],hparams['select'],hparams['scale'], str(hparams['epochs']), str(hparams['save_step']), str(hparams['his_size']), str(hparams['k']))\n",
    "        # f.write(model_name + '\\n')\n",
    "        d = {}\n",
    "        for k,v in hparams.items():\n",
    "            if k in hparam_list:\n",
    "                d[k] = v\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in param_list:\n",
    "                d[name] = tuple(param.shape)\n",
    "\n",
    "        f.write(str(d)+'\\n')\n",
    "        f.write(str(res) +'\\n')\n",
    "        f.write('\\n')        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}