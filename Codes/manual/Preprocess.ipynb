{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnnconda048fc75fe4ee43f1aa97608c8881ebba",
   "display_name": "Python 3.8.5 64-bit ('nn': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import prepare,analyse\n",
    "\n",
    "hparams = {\n",
    "    'npratio':4,\n",
    "    'scale':'demo',\n",
    "    'batch_size':10,\n",
    "    'his_size':50,\n",
    "    'title_size':15,\n",
    "    'device':'cuda:1',\n",
    "    'attrs': ['title'],\n",
    "    'news_id':True,\n",
    "    'k': ''\n",
    "}\n",
    "\n",
    "device = torch.device(hparams['device'])\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loader_train, loader_test = prepare(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze MIND Datasets\n",
    "- average title length\n",
    "- average abstract length\n",
    "- average history length\n",
    "- average impression capacity\n",
    "- count of history exceeding 50\n",
    "- count of empty history\n",
    "- count of multi-clicked impressions "
   ]
  },
  {
   "source": [
    "hparams['scale'] = 'large'\n",
    "hparams['mode'] = 'tra'\n",
    "analyse(hparams)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tailor Data to demo size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tailorData('/home/peitian_zhang/Data/MIND/MINDsmall_dev/behaviors.tsv',500)\n",
    "tailorData('/home/peitian_zhang/Data/MIND/MINDsmall_train/behaviors.tsv',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/peitian_zhang/Data/MIND/MINDlarge_test/news.tsv','r',encoding='utf-8')\n",
    "\n",
    "nid2index = {}\n",
    "for line in f:\n",
    "    nid,_,_,_,_,_,_,_ = line.strip(\"\\n\").split('\\t')\n",
    "\n",
    "    if nid in nid2index:\n",
    "        continue\n",
    "    nid2index[nid] = len(nid2index) + 1\n",
    "\n",
    "f.close()\n",
    "h = open('/home/peitian_zhang/Codes/News-Recommendation/data/dictionaries/nid2idx_large_dev.json','w',encoding='utf-8')\n",
    "json.dump(nid2index,h,ensure_ascii=False)\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('/home/peitian_zhang/Data/MIND/MINDlarge_test/behaviors.tsv','r',encoding='utf-8')\n",
    "\n",
    "uid2index = {}\n",
    "for line in f:\n",
    "    _,uid,_,_,_ = line.strip(\"\\n\").split('\\t')\n",
    "\n",
    "    if uid in uid2index:\n",
    "        continue\n",
    "    uid2index[uid] = len(uid2index) + 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "h = open('/home/peitian_zhang/Codes/News-Recommendation/data/dictionaries/uid2idx_large_dev.json','w',encoding='utf-8')\n",
    "json.dump(uid2index,h,ensure_ascii=False)\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructVocab(['/home/peitian_zhang/Data/MIND/MINDlarge_dev/news.tsv','/home/peitian_zhang/Data/MIND/MINDlarge_test/news.tsv','/home/peitian_zhang/Data/MIND/MINDlarge_train/news.tsv'], '/home/peitian_zhang/Codes/News-Recommendation/data/dictionaries/vocab_demo_title_category_subcategory.pkl', ['title','category','subcategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import constructNid2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructNid2idx('/home/peitian_zhang/Data/MIND/MINDsmall_train/news.tsv','/home/peitian_zhang/Data/MIND/MINDsmall_dev/news.tsv','small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructNid2idx('/home/peitian_zhang/Data/MIND/MINDlarge_train/news.tsv','/home/peitian_zhang/Data/MIND/MINDlarge_dev/news.tsv','large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructNid2idx('/home/peitian_zhang/Data/MIND/MINDdemo_train/news.tsv','/home/peitian_zhang/Data/MIND/MINDdemo_dev/news.tsv','demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset,IterableDataset, DataLoader, get_worker_info\n",
    "from utils.utils import newsample,getId2idx,word_tokenize_vocab,getVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIND_news(Dataset):\n",
    "    \"\"\" Map style dataset\n",
    "\n",
    "    Args:\n",
    "        hparams(dict): pre-defined dictionary of hyper parameters\n",
    "        mode(str): train/test\n",
    "        news_file(str): path of news_file\n",
    "        behaviors_file(str): path of behaviors_file\n",
    "    \"\"\"\n",
    "    def __init__(self,hparams,news_file,col_spliter='\\t'):\n",
    "        # initiate the whole iterator\n",
    "        self.npratio = hparams['npratio']\n",
    "        self.news_file = news_file\n",
    "        self.col_spliter = col_spliter        \n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.title_size = hparams['title_size']\n",
    "        self.his_size = hparams['his_size']\n",
    "        self.attrs = hparams['attrs']\n",
    "        self.k = hparams['k']\n",
    "\n",
    "        self.vocab = getVocab('data/dictionaries/vocab_{}_{}.pkl'.format(hparams['scale'],'_'.join(hparams['attrs'])))\n",
    "        self.nid2index = getId2idx('data/dictionaries/nid2idx_{}_train.json'.format(hparams['scale']))\n",
    "        self.uid2index = getId2idx('data/dictionaries/uid2idx_{}.json'.format(hparams['scale']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        if not hasattr(self, \"news_title_array\"):\n",
    "            self.init_news()\n",
    "\n",
    "        return len(self.news_title_array)\n",
    "    \n",
    "    def init_news(self):\n",
    "        \"\"\" \n",
    "            init news information given news file, such as news_title_array.\n",
    "        \"\"\"\n",
    "\n",
    "        title_token = []\n",
    "        # category_token = [[0]]\n",
    "        # subcategory_token = [[0]]\n",
    "\n",
    "        title_pad = [[self.title_size]]\n",
    "        \n",
    "        with open(self.news_file,\"r\",encoding='utf-8') as rd:\n",
    "\n",
    "            for idx in rd:\n",
    "                nid, vert, subvert, title, ab, url, _, _ = idx.strip(\"\\n\").split(\n",
    "                    self.col_spliter\n",
    "                )\n",
    "\n",
    "                title = word_tokenize_vocab(title,self.vocab)\n",
    "                title_token.append(title[:self.title_size] + [0] * (self.title_size - len(title)))\n",
    "                title_pad.append([max(self.title_size - len(title), 0)])\n",
    "                # category_token.append([self.vocab[vert]])\n",
    "                # subcategory_token.append([self.vocab[subvert]])\n",
    "        \n",
    "        self.news_title_array = np.asarray(title_token)\n",
    "        # self.news_category_array = np.asarray(category_token)\n",
    "        # self.news_subcategory_array = np.asarray(subcategory_token)\n",
    "\n",
    "        self.title_pad = np.asarray(title_pad)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" parse behavior log No.idx to training example\n",
    "\n",
    "        Args:\n",
    "            idx (int): impression index, start from zero\n",
    "\n",
    "        Returns:\n",
    "            dict of training data, including |npratio+1| candidate news word vector, |his_size+1| clicked news word vector etc.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"news_title_array\"):\n",
    "            self.init_news()\n",
    "\n",
    "        return {\n",
    "            \"candidate_title\": self.news_title_array[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind = MIND_news(hparams,'/home/peitian_zhang/Data/MIND/MINDdemo_train/news.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'candidate_title': array([ 345,  698, 3561,    9, 3803, 3033,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "mind.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}